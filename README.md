# NNscratch
Creating an MNIST NN from scratch without any external libraries except NumPy

https://mlfromscratch.com/neural-network-tutorial/#/

http://yann.lecun.com/exdb/mnist/

# Structure

 - Input layer: flattened 28x28=784
 - First hidden layer: reduce to 16x16=256
 - Second hidden layer: reduce to 8x8 = 128
 - Output layer: reduce to last 10

 Note to self: where to get the layer sizes? Usually empirical evidence.


 # Todo

  - Softmax derivative
  - Backpropagation
  - Testing the net
  - Write everything open: what is actually happening in each step?
